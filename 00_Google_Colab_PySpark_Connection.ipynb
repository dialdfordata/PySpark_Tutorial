{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3ROHt9ujr6YqzVu1gq3/o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dialdfordata/PySpark_Tutorial/blob/main/00_Google_Colab_PySpark_Connection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PySpark on Google Colab Environment**\n",
        "\n",
        "In this Notebook we will go thorugh a series of task that we need to perform to run PySpark in the Google Colab Environment.\n",
        "\n",
        "As you know, Google Colab is an online notebook-like coding environment that is well-suited for machine learning and data analysis.\n",
        "\n",
        "It comes equipped with many Machine Learning libraries and offers even free GPU usage. It is mainly used by data scientists and ML engineers."
      ],
      "metadata": {
        "id": "KPypDBauxnha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first install Open JDK or Jave Development Kit. Apache Spark is written in Scala, which runs on the Java Virtual Machine (JVM). OpenJDK provides an open source implementation of the JVM, ensuring compatibility and support for running Spark."
      ],
      "metadata": {
        "id": "81yQTMfYpANQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the backend of the Google Colab we have a Linux system, so we will use some Linux commands for installation Open JDK, first we will write a command  ``!sudo apt update`` which will update the package lists from the repositories to ensure that the latest inofration about available packages can be retrieved."
      ],
      "metadata": {
        "id": "nKaILef4pZCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open JDK installation\n",
        "\n",
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "tBsM4dfKvN-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will install opnejdk version 8.\n",
        "\n",
        "``!apt-get install openjdk-8-jdk-headless -qq > /dev/null`` :\n",
        "\n",
        "This command will install the headless version of OpenJDK 8 quietly hence   (``-qq``) without user any interaction, suppressing most output by redirecting it to ``/dev/null``. The \"headless\" version excludes graphical components, making it suitable for server environments or automated tasks."
      ],
      "metadata": {
        "id": "V8GXaHCQ4BV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to install spark-3.5.3 version that is available in Apache Spark official website."
      ],
      "metadata": {
        "id": "FRk7Cz-Sq6lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download spark-3.5.3-bin-hadoop3, a specific distribution of Apache Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "P_AMYtF3vTbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block uses the ``wget`` command to download a file from the specified URL. The -q flag enables quiet mode, suppressing most output to the terminal. Specifically, it retrieves the Apache Spark version 3.5.3 binary package for Hadoop 3, compressed in a ``.tgz`` archive, from the provided Apache mirror URL"
      ],
      "metadata": {
        "id": "PLt5ToYi5-4U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to unpack the downloaded spark file."
      ],
      "metadata": {
        "id": "1D24bELoraPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unpack the contents of the spark-3.5.3-bin-hadoop3.tgz file into the file system\n",
        "!tar xf spark-3.5.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "oLuHi5X1vVzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block extracts the contents of the ``spark-3.5.3-bin-hadoop3.tgz`` archive using the tar command. The xf options specify that the archive should be e**x**tracted (``x``) and that the archive's **f**ile name (``spark-3.5.3-bin-hadoop3.tgz``) follows. This command unpacks the Spark binary distribution into the current working directory, making its files and directories accessible for use. It is commonly used when setting up Apache Spark for data processing tasks."
      ],
      "metadata": {
        "id": "iKBSW7GK5sNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, we have downloaded and installed the Spark in the system, next we need to define to variables, one for Java and another for Spark. The JAVA_HOME environment variable points to the Java installation directory on the machine and is essential for Spark and the SPARK_HOME environment variable points to the Apache Spark installation directory. It is used by Spark to localize its own components and libraries."
      ],
      "metadata": {
        "id": "IDOiigRssDB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration of environment variables\n",
        "# The JAVA_HOME environment variable points to the Java installation directory on the machine and is essential for Spark\n",
        "# The SPARK_HOME environment variable points to the Apache Spark installation directory. It is used by Spark to localize its own components and libraries.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = f\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/spark-3.5.3-bin-hadoop3\""
      ],
      "metadata": {
        "id": "uTxvar5vvcRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further, we need to install three Python libraries, first one is 'findspark'. The findspark library is used to make it easier to configure and launch Apache Spark in local environments, such as on this development machines. We also need to install py4j or Python for Java and upated version of pyspark."
      ],
      "metadata": {
        "id": "nK3OCDMPs1Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation of the required libaries\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install py4j -q\n",
        "!pip install pyspark -qU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-n3spnMvgDH",
        "outputId": "a1dc405d-bef8-4951-94dd-814e565f0126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to import findspark library which is a utility for simplifying the process of setting up PySpark (Spark's Python API) in environments like Jupyter Notebooks."
      ],
      "metadata": {
        "id": "A3izMJtuuFZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# findspark.init() makes it easy to configure and launch Apache Spark in local development environments\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "ySFt-EhRze3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to initalize the findspark library with ``findspark.init()`` method, initializes ``findspark``, will add the Apache Spark installation directory to the ``PYTHONPATH`` and setting the necessary environment variables. This ensures that PySpark modules can be imported and Spark-related commands can run without manual configuration of paths."
      ],
      "metadata": {
        "id": "sAp_oU4D5OxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we need to import the pyspark library."
      ],
      "metadata": {
        "id": "3RIdsZONvAKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "0I3jJL3Jyhz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since, the pyspark libray is imported successfully, now we will create a Spark Session which is the entry point for the Spark program, I will make a separate video to explain Spark Context and Spark Session. For the time being follow along with me."
      ],
      "metadata": {
        "id": "v_5HaTm8vKlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= (SparkSession\n",
        "        .builder\n",
        "        .appName(\"PySaprk Connection in Google Colab\")\n",
        "        .getOrCreate()\n",
        ")\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "4Q_wxkG4vnJx",
        "outputId": "f968f98b-b61d-4780-86b1-eb1d78b1bf9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bffa59354e0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d1bf60be0e00:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Our First Spark Example</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect!! Our Spark session is up and running successfully. Now, we will create a small Spark Data frame to see our PySpark in live action.\n",
        "\n",
        "First we need to import the DataFrame method from spark dot sql\n",
        "\n",
        "Then will defne the data and the columns and then finally we create the Spark Data Frame with spark.createDataFrame method.\n",
        "\n",
        "df.show will print the data frame on the screen now."
      ],
      "metadata": {
        "id": "_nNyHgvqvx-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Data\n",
        "\n",
        "from pyspark.sql import DataFrame\n",
        "\n",
        "# Definig the data\n",
        "data = [\n",
        "    (1,\"Jane\", \"Admin\"),\n",
        "    (2, \"Joe\", \"HR\"),\n",
        "    (3, \"John\", \"IT\"),\n",
        "    (4, \"Mary\", \"Legal\"),\n",
        "    (5, \"Kate\", \"IT\")\n",
        "]\n",
        "\n",
        "# Defining the columns\n",
        "columns = [\"id\", \"name\", \"dept\"]\n",
        "\n",
        "# Creation of Data Frame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvUCFsa_v_3T",
        "outputId": "b2820307-fe7b-4bb6-c31d-2475053294bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+--------+\n",
            "| id|  name|    dept|\n",
            "+---+------+--------+\n",
            "|  1|  Neil|Purchase|\n",
            "|  2|Donald|   Admin|\n",
            "|  3|  John|      IT|\n",
            "|  4|  Mary|      HR|\n",
            "+---+------+--------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}